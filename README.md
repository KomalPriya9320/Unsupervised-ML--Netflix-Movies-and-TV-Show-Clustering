
</p>
<h1 align="center"> Netflix-Movies-and-TV-Shows-Clustering </h1>
<h3 align="center"> AlmaBetter Verfied Project - <a href="https://www.almabetter.com/"> AlmaBetter School </a> </h5>

![Screenshot (33)](https://user-images.githubusercontent.com/102009481/177919568-3616b07c-f7d2-4d58-bc98-ab10b39fbfee.png)






<p>Netflix Recommender recommends Netflix movies and TV shows based on a user's favorite movie or TV show. It uses a  a K-Means Clustering model to make these recommendations. These models use information about movies and TV shows such as their plot descriptions and genres to make suggestions..</p>

<h2> :floppy_disk: Project Files Description</h2>


<p>This Project includes 1 executable files and Dataset directories as follows:</p>
<h4>Executable Files:</h4>
<ul>
  
  <li><b>Komal_final_notebook_NETFLIX_MOVIES_AND_TV_SHOWS_CLUSTERING.ipynb</b> - Includes all functions required for clustering operations  and generates the model.h5 file after execution.</li>
  
<h4>Source Directories:</h4>
<ul>
  <li><b>Dataset</b> - Includes all dataset  for the training phase  and testing phase of the model in the csv format.</li>
  
</ul>


  
</ul>

![-----------------------------------------------------](https://raw.githubusercontent.com/andreasbm/readme/master/assets/lines/rainbow.png)

<h2> :book: Kmeans clustering </h2>

<p> k-means clustering is a method of vector quantization, originally from signal processing, that aims to partition n observations into k clusters in which each observation belongs to the cluster with the nearest mean (cluster centers or cluster centroid), serving as a prototype of the cluster. This results in a partitioning of the data space into Voronoi cells. k-means clustering minimizes within-cluster variances (squared Euclidean distances), but not regular Euclidean distances, which would be the more difficult Weber problem: the mean optimizes squared errors, whereas only the geometric median minimizes Euclidean distances. For instance, better Euclidean solutions can be found using k-medians and k-medoids.
  
  ![Screenshot (34)](https://user-images.githubusercontent.com/102009481/177921083-3eef47d2-43d8-4c87-a027-5aebd7ba7304.png)

   
  

<h2> :book: Agglomerative Hierarchical clustering </h2>

<p> The agglomerative hierarchical clustering algorithm is a popular example of HCA. To group the datasets into clusters, it follows the bottom-up approach. It means, this algorithm considers each dataset as a single cluster at the beginning, and then start combining the closest pair of clusters together. It does this until all the clusters are merged into a single cluster that contains all the datasets.

This hierarchy of clusters is represented in the form of the dendrogram.
  
  ![Screenshot (35)](https://user-images.githubusercontent.com/102009481/177921624-6f20d71d-cd54-4d56-a7b3-3e2399c14426.png)








![-----------------------------------------------------](https://raw.githubusercontent.com/andreasbm/readme/master/assets/lines/rainbow.png)

<h2> :clipboard: Execution Instruction</h2>
<p>The order of execution of the program files is as follows:</p>


<p><b>1) Komal_final_notebook_NETFLIX_MOVIES_AND_TV_SHOWS_CLUSTERING.ipynb</b></p>
<p> This file must be executed, to define all the functions and variables required for regression operations which leads to the production of the model.h5 file. and to evaluate the model performance on unseen data

